{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75560e7a",
   "metadata": {},
   "source": [
    "# Map Segmentation (U-Net) â€” End-to-end notebook\n",
    "\n",
    "**What this notebook does:**\n",
    "\n",
    "- Loads paired satellite images and masks (robust pairing by base filename)\n",
    "- Preprocesses and augments data (masks use nearest resampling)\n",
    "- Builds a U-Net model, trains with BCE + Dice loss, and monitors Dice coefficient\n",
    "- Provides inference, post-processing, and visualization utilities\n",
    "\n",
    "**How to use:** edit `TRAIN_DIR` and `TEST_DIR` to point to your dataset folders. Image files should follow a naming convention like `0001_sat.jpg` and masks `0001_mask.png`.\n",
    "\n",
    "Run all cells sequentially.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b836a53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, backend as K\n",
    "import json\n",
    "from tensorflow.keras.optimizers.experimental import AdamW\n",
    "print('tf version', tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7931d300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Utilities: reading images and masks ----------\n",
    "def read_image_rgb(path, shape):\n",
    "    img = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n",
    "    if img is None:\n",
    "        raise FileNotFoundError(f'Image not found: {path}')\n",
    "    if img.ndim == 2:  # grayscale -> convert to 3 channel\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
    "    elif img.shape[-1] == 4:  # BGRA -> BGR\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGRA2BGR)\n",
    "    # convert BGR -> RGB\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, (shape, shape), interpolation=cv2.INTER_AREA)\n",
    "    img = img.astype(np.float32) / 255.0\n",
    "    return img\n",
    "\n",
    "def read_mask(path, shape):\n",
    "    mask = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "    if mask is None:\n",
    "        raise FileNotFoundError(f'Mask not found: {path}')\n",
    "    mask = cv2.resize(mask, (shape, shape), interpolation=cv2.INTER_NEAREST)\n",
    "    mask = (mask > 127).astype(np.float32)\n",
    "    mask = np.expand_dims(mask, axis=-1)\n",
    "    return mask\n",
    "\n",
    "def LoadData_pairwise(img_dir, mask_dir=None, shape=128, img_suffix='_sat.jpg', mask_suffix='_mask.png'):\n",
    "    \"\"\"Pair images and masks by base filename. Returns dict with 'img' and 'mask' numpy arrays.\"\"\"\n",
    "    if mask_dir is None:\n",
    "        mask_dir = img_dir\n",
    "\n",
    "    img_files = [f for f in os.listdir(img_dir) if f.endswith(img_suffix)]\n",
    "    mask_files = [f for f in os.listdir(mask_dir) if f.endswith(mask_suffix)]\n",
    "\n",
    "    img_map = {}\n",
    "    for f in img_files:\n",
    "        base = os.path.splitext(f)[0].replace('_sat','').rstrip('_')\n",
    "        img_map[base] = os.path.join(img_dir, f)\n",
    "\n",
    "    mask_map = {}\n",
    "    for f in mask_files:\n",
    "        base = os.path.splitext(f)[0].replace('_mask','').rstrip('_')\n",
    "        mask_map[base] = os.path.join(mask_dir, f)\n",
    "\n",
    "    common = sorted(set(img_map.keys()) & set(mask_map.keys()))\n",
    "    print(f'Found {len(common)} paired examples.')\n",
    "\n",
    "    images = []\n",
    "    masks = []\n",
    "    for base in common:\n",
    "        try:\n",
    "            images.append(read_image_rgb(img_map[base], shape))\n",
    "            masks.append(read_mask(mask_map[base], shape))\n",
    "        except Exception as e:\n",
    "            print('Skipping', base, 'because', e)\n",
    "\n",
    "    images = np.array(images, dtype=np.float32)\n",
    "    masks = np.array(masks, dtype=np.float32)\n",
    "    return {'img': images, 'mask': masks}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4fc065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Augmentation (tf.data friendly) ----------\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "    layers.RandomContrast(0.2),\n",
    "    layers.RandomZoom((-0.15, 0.0)),\n",
    "    layers.RandomBrightness(0.1),\n",
    "], name=\"data_augmentation\")\n",
    "\n",
    "def augment_fn(image, mask):\n",
    "    stacked = tf.concat([image, mask], axis=-1)\n",
    "    stacked = data_augmentation(stacked)\n",
    "    image_aug = stacked[..., :3]\n",
    "    mask_aug = stacked[..., 3:]\n",
    "    \n",
    "    # Ensure mask stays binary after augmentation\n",
    "    mask_aug = tf.cast(mask_aug > 0.5, tf.float32)\n",
    "    \n",
    "    return image_aug, mask_aug\n",
    "  \n",
    "def make_dataset(frames, batch_size=16, augment=True, shuffle=True):\n",
    "    images = frames['img']\n",
    "    masks = frames['mask']\n",
    "    ds = tf.data.Dataset.from_tensor_slices((images, masks))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=max(1000, len(images)))\n",
    "    if augment:\n",
    "        ds = ds.map(lambda x,y: augment_fn(x,y), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    ds = ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab6733e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- U-Net model definition ----------\n",
    "def Conv2dBlock(inputTensor, numFilters, kernelSize=3, doBatchNorm=True):\n",
    "    x = layers.Conv2D(filters=numFilters, kernel_size=(kernelSize, kernelSize),\n",
    "                      kernel_initializer='he_normal', padding='same')(inputTensor)\n",
    "    if doBatchNorm:\n",
    "        x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "\n",
    "    x = layers.Conv2D(filters=numFilters, kernel_size=(kernelSize, kernelSize),\n",
    "                      kernel_initializer='he_normal', padding='same')(x)\n",
    "    if doBatchNorm:\n",
    "        x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "def unetBlock(inputImage, numFilters=16, droupouts=0.1, doBatchNorm=True):\n",
    "    c1 = Conv2dBlock(inputImage, numFilters * 1, kernelSize=3, doBatchNorm=doBatchNorm)\n",
    "    p1 = layers.MaxPooling2D((2,2))(c1)\n",
    "    p1 = layers.Dropout(droupouts)(p1)\n",
    "\n",
    "    c2 = Conv2dBlock(p1, numFilters * 2, kernelSize=3, doBatchNorm=doBatchNorm)\n",
    "    p2 = layers.MaxPooling2D((2,2))(c2)\n",
    "    p2 = layers.Dropout(droupouts)(p2)\n",
    "\n",
    "    c3 = Conv2dBlock(p2, numFilters * 4, kernelSize=3, doBatchNorm=doBatchNorm)\n",
    "    p3 = layers.MaxPooling2D((2,2))(c3)\n",
    "    p3 = layers.Dropout(droupouts)(p3)\n",
    "\n",
    "    c4 = Conv2dBlock(p3, numFilters * 8, kernelSize=3, doBatchNorm=doBatchNorm)\n",
    "    p4 = layers.MaxPooling2D((2,2))(c4)\n",
    "    p4 = layers.Dropout(droupouts)(p4)\n",
    "\n",
    "    c5 = Conv2dBlock(p4, numFilters * 16, kernelSize=3, doBatchNorm=doBatchNorm)\n",
    "\n",
    "    u6 = layers.Conv2DTranspose(numFilters*8, (3,3), strides=(2,2), padding='same')(c5)\n",
    "    u6 = layers.concatenate([u6, c4])\n",
    "    u6 = layers.Dropout(droupouts)(u6)\n",
    "    c6 = Conv2dBlock(u6, numFilters * 8, kernelSize=3, doBatchNorm=doBatchNorm)\n",
    "\n",
    "    u7 = layers.Conv2DTranspose(numFilters*4, (3,3), strides=(2,2), padding='same')(c6)\n",
    "    u7 = layers.concatenate([u7, c3])\n",
    "    u7 = layers.Dropout(droupouts)(u7)\n",
    "    c7 = Conv2dBlock(u7, numFilters * 4, kernelSize=3, doBatchNorm=doBatchNorm)\n",
    "\n",
    "    u8 = layers.Conv2DTranspose(numFilters*2, (3,3), strides=(2,2), padding='same')(c7)\n",
    "    u8 = layers.concatenate([u8, c2])\n",
    "    u8 = layers.Dropout(droupouts)(u8)\n",
    "    c8 = Conv2dBlock(u8, numFilters * 2, kernelSize=3, doBatchNorm=doBatchNorm)\n",
    "\n",
    "    u9 = layers.Conv2DTranspose(numFilters*1, (3,3), strides=(2,2), padding='same')(c8)\n",
    "    u9 = layers.concatenate([u9, c1])\n",
    "    u9 = layers.Dropout(droupouts)(u9)\n",
    "    c9 = Conv2dBlock(u9, numFilters * 1, kernelSize=3, doBatchNorm=doBatchNorm)\n",
    "\n",
    "    output = layers.Conv2D(1, (1,1), activation='sigmoid')(c9)\n",
    "    model = tf.keras.Model(inputs=[inputImage], outputs=[output])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fe25d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Losses and metrics (BCE + Dice) ----------\n",
    "def dice_coef(y_true, y_pred, smooth=1e-6):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    return 1.0 - dice_coef(y_true, y_pred)\n",
    "\n",
    "def bce_dice_loss(y_true, y_pred):\n",
    "    bce = tf.keras.losses.BinaryCrossentropy()(y_true, y_pred)\n",
    "    return bce + dice_loss(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96631739",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exact_count_dataset_images(dataset):\n",
    "    total_images = 0\n",
    "    for images, masks in dataset:\n",
    "        batch_size = images.shape[0]\n",
    "        total_images += batch_size\n",
    "    return total_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58e4bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Training setup & run ----------\n",
    "# EDIT THESE PATHS before running\n",
    "TRAIN_DIR = 'data/train'     # Your train folder\n",
    "VALID_DIR = 'data/valid'     # Your validation folder\n",
    "MASK_DIR = None              # set to None if masks are in same folders\n",
    "\n",
    "IMG_SIZE = 128\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 83\n",
    "\n",
    "train_frames = LoadData_pairwise(TRAIN_DIR, mask_dir=MASK_DIR, shape=IMG_SIZE)\n",
    "print('Train - images shape:', train_frames['img'].shape, 'masks shape:', train_frames['mask'].shape)\n",
    "\n",
    "valid_frames = LoadData_pairwise(VALID_DIR, mask_dir=MASK_DIR, shape=IMG_SIZE)\n",
    "print('Valid - images shape:', valid_frames['img'].shape, 'masks shape:', valid_frames['mask'].shape)\n",
    "\n",
    "train_ds = make_dataset(train_frames, batch_size=BATCH_SIZE, augment=True, shuffle=True)\n",
    "valid_ds = make_dataset(valid_frames, batch_size=BATCH_SIZE, augment=False, shuffle=False)\n",
    "\n",
    "\n",
    "print(\"=== EXACT COUNTS ===\")\n",
    "print(f\"Training - Original: {len(train_frames['img'])}, Augmented per epoch: {exact_count_dataset_images(train_ds)}\")\n",
    "print(f\"Validation - Original: {len(valid_frames['img'])}, Augmented per epoch: {exact_count_dataset_images(valid_ds)}\")\n",
    "\n",
    "print(\"=== EXACT COUNTS ===\")\n",
    "print(f\"Training - Original: {len(train_frames['img'])}, Augmented per epoch: {exact_count_dataset_images(train_ds)}\")\n",
    "print(f\"Validation - Original: {len(valid_frames['img'])}, Augmented per epoch: {exact_count_dataset_images(valid_ds)}\")\n",
    "\n",
    "inputs = layers.Input((IMG_SIZE, IMG_SIZE, 3))\n",
    "model = unetBlock(inputs, droupouts=0.05)\n",
    "model.compile(\n",
    "    optimizer=AdamW(1e-3, weight_decay=1e-4),\n",
    "    loss=bce_dice_loss,\n",
    "    metrics=[dice_coef, tf.keras.metrics.BinaryAccuracy(), \n",
    "             tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=6, min_lr=1e-7, verbose=1),\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=1),\n",
    "    tf.keras.callbacks.ModelCheckpoint('best_model.keras', monitor='val_dice_coef', mode='max', save_best_only=True, verbose=1),\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds, \n",
    "    validation_data=valid_ds,\n",
    "    epochs=EPOCHS, \n",
    "    callbacks=callbacks, \n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91784861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Plot training history ----------\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "    axes[0,0].plot(history.history['loss'], label='Train Loss')\n",
    "    axes[0,0].plot(history.history['val_loss'], label='Val Loss')\n",
    "    axes[0,0].set_title('Loss')\n",
    "    axes[0,0].legend()\n",
    "    axes[0,0].grid(True)\n",
    "\n",
    "    axes[0,1].plot(history.history['dice_coef'], label='Train Dice')\n",
    "    axes[0,1].plot(history.history['val_dice_coef'], label='Val Dice')\n",
    "    axes[0,1].set_title('Dice Coefficient')\n",
    "    axes[0,1].legend()\n",
    "    axes[0,1].grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print('No history object found:', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8aab40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Inference + postprocessing ----------\n",
    "def postprocess_mask(pred, min_area=50):\n",
    "    m = (pred > 0.5).astype('uint8')\n",
    "    if m.ndim == 3 and m.shape[-1] == 1:\n",
    "        m = np.squeeze(m, axis=-1)\n",
    "    num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(m, connectivity=8)\n",
    "    out = np.zeros_like(m, dtype='uint8')\n",
    "    for i in range(1, num_labels):\n",
    "        if stats[i, cv2.CC_STAT_AREA] >= min_area:\n",
    "            out[labels==i] = 1\n",
    "    return out\n",
    "\n",
    "def predict_image(model, image_path, img_size=128):\n",
    "    img = read_image_rgb(image_path, img_size)\n",
    "    pred = model.predict(np.expand_dims(img, axis=0))[0,...,0]\n",
    "    return pred\n",
    "\n",
    "# Example usage (edit paths if you have test images)\n",
    "TEST_IMAGES = ['data/test/1_sat.jpg']  # modify as needed\n",
    "for p in TEST_IMAGES:\n",
    "    try:\n",
    "        pred = predict_image(model, p, IMG_SIZE)\n",
    "        proc = postprocess_mask(pred, min_area=30)\n",
    "        # show results\n",
    "        orig = read_image_rgb(p, IMG_SIZE)\n",
    "        plt.figure(figsize=(8,4))\n",
    "        plt.subplot(1,3,1); plt.imshow(orig); plt.axis('off'); plt.title('orig')\n",
    "        plt.subplot(1,3,2); plt.imshow(pred, vmin=0, vmax=1); plt.axis('off'); plt.title('raw pred')\n",
    "        plt.subplot(1,3,3); plt.imshow(proc, cmap='gray'); plt.axis('off'); plt.title('postproc')\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print('Skipping', p, 'because', e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
